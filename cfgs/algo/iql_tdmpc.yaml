# @package _global_

agent:
  _target_: algorithm.iql_tdmpc.IQLTDMPC
  # Whether to use the `V` critic vs. the `Q` critic to estimate the value of a
  # state (used during planning on the last predicted state at timestep `horizon`).
  use_v_critic_for_state_value: false
  # Whether to learn a `Q` critic associated to the policy that we learn:
  #   - "do_not_use": do not learn it ("regular" IQL)
  #   - "use_only_for_planning": learn it and use it only to bootstrap Q-values
  #     during planning (this is incompatible with `use_v_critic_for_state_value`)
  #   - "replace_iql_critic": learn it and use it everywhere instead of the regular
  #     IQL `Q` critic (i.e., both for planning and to learn `V` & advantages)
  use_q_pi: do_not_use

stochastic_policy:
  # Whether the entropy loss terms (for the policy and to learn alpha) should be
  # weighted by the exponentiated advantages in the IQL loss.
  weighted_entropy_loss: true

# Loss coefficients.
critic_q_pi_coef: 0.1
critic_v_coef: 0.1
offline: true

iql:
  # Inverse temperature for policy optimization.
  # Small beta => Behavior Cloning, high beta => maximizing Q
  beta: 3.0
  # Coefficient for the asymmetric loss to update V.
  tau: 0.7
  # Maximum value of the exponentiated advantage used for policy optimization.
  max_exp_advantage: 100.